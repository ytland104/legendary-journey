{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting access to web and scrape the html using def\n",
    "\n",
    "import hashlib\n",
    "import io\n",
    "import os\n",
    "import re  # get only jpg extension\n",
    "import time\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def Folder_Path(q):\n",
    "    make_dir =\"./Picture/Bijyo/\"+str(q)+'/'\n",
    "    os.makedirs(make_dir,exist_ok=True)\n",
    "    return make_dir\n",
    "\n",
    "def Scraping_picture(img,fold_number): \n",
    "     \n",
    "#     for img in jpg_link[:]:\n",
    "        \n",
    "    try:\n",
    "        image_content=requests.get(img).content\n",
    "    except Exception as e:\n",
    "        print(f\"Error - could not get image {img} - {e}\") \n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        file_path = os.path.join(Folder_Path(fold_number),hashlib.sha1(image_content).hexdigest()[:10] + \".jpg\",)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "                image.save(f, \"JPEG\", quality=100)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save picture {img} - {e}\")\n",
    "\n",
    "def get_jpeglink(urls,fold_number):\n",
    "    \n",
    "    html = requests.get(urls)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "    html_p = soup.find_all(class_='content')\n",
    "    \n",
    "    for html_p in soup.find_all('a',attrs={'href': re.compile('.jpg')}):\n",
    "#         print(\"Found the URL:\", html_p['href'])\n",
    "#         time.sleep(1.5)\n",
    "\n",
    "        Scraping_picture(html_p['href'],fold_number)\n",
    "    \n",
    "   \n",
    "#     print(html_p)\n",
    "#     for item in html_p:\n",
    "#         next_links = item.find_all('a')\n",
    "# #         next_links = item.find_all('a',attrs={'href': re.compile('(https?://[^\\s]+.[\\d]*\\.jpg)')})\n",
    "# #         images = bs.find_all('img', attrs={'src': re.compile('.jpg')})\n",
    "#         print(next_links)\n",
    "\n",
    "#         next_link=[tag.get('href') for tag in next_links]\n",
    "#         Scraping_picture(next_link,fold_number)\n",
    "                \n",
    "def get_weblinks(load_url):\n",
    "    fold_number=load_url.split('/')[-1]\n",
    "    \n",
    "    html = requests.get(load_url)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    html_t = soup.find_all(class_='title')\n",
    "#     ttl = [x.get_text() for x in soup.find_all('div', attrs={'class':'title'})]\n",
    "#     print(html_t[0])\n",
    "    try:\n",
    "        m=re.findall(r'[\\d]*枚',str(html_t[0]))[0].replace('枚','') #枚数が70枚以上ならページを５ページ回す。\n",
    "        print(f'{load_url} - {m}')\n",
    "\n",
    "        if int(m) > 60:\n",
    "            pgs = int(m) // 60 \n",
    "            for p in range(1,pgs):\n",
    "                url = load_url+f\"/{p}\"\n",
    "                get_jpeglink(url,fold_number)\n",
    "        else:\n",
    "            get_jpeglink(load_url,fold_number)\n",
    "    except Exception as e:\n",
    "        get_jpeglink(load_url,fold_number)\n",
    "    \n",
    "\n",
    "#     return next_link\n",
    "\n",
    "# q=1\n",
    " #12\n",
    "for i in range(1,23):\n",
    "#     url='https://erogazou.gallery/tag/%E7%BE%8E%E4%B9%B3/page/{}'.format(i)\n",
    "#     url='https://erogazou.gallery/category/%e3%82%b0%e3%83%a9%e3%83%93%e3%82%a2%e3%83%bb%e3%83%8c%e3%83%bc%e3%83%89/page/{}'.format(i)\n",
    "    url='https://erogazou.gallery/category/%e3%81%8a%e3%81%a3%e3%81%b1%e3%81%84/page/{}'.format(i)\n",
    "    url='https://erogazou.gallery/tag/%e7%be%8e%e5%a5%b3/page/{}'.format(i)\n",
    "#     url ='http://hnalady.com/blog-entry-{}.html'.format(i)\n",
    "#     url ='https://www.matome-blog.site/?p={}'.format(i)\n",
    "#     url ='https://news.idolsenka.net/?p={}#more-{}'.format(i,i)\n",
    "#     url ='https://1000giribest.com/282059.html'\n",
    "#   \n",
    "    print(url)\n",
    "    \n",
    "    get_picture=[] \n",
    "\n",
    "    try:\n",
    "        req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        webpage = urlopen(req).read()\n",
    "        html = soup(webpage,'lxml')\n",
    "#         print(html)\n",
    "        html_p = html.find_all(class_='thumb-box sub-color-background')   #content  selectionsure ui-sortable-handle\n",
    "#         html_p = html.find_all(class_='entry-content')\n",
    "#         html_p = html.find_all(class_='content')\n",
    "        for item in html_p:\n",
    "            links = item.find_all('a')  #----- \n",
    "#             jpg_link=[tag.get('href') for tag in links] #-----\n",
    "            web_link=[tag.get('href') for tag in links] #-----\n",
    "#             print(web_link[0])\n",
    "            get_picture.append(web_link[0])  #リンクをリスト化。\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")\n",
    "        \n",
    "    print(get_picture)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        for html in get_picture:\n",
    "            get_weblinks(html)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not analysis {url} - {e}\")\n",
    "    \n",
    "\n",
    "#     Target_path=Folder_Path(i)\n",
    "#     Scraping_picture(jpg_link,Target_path)\n",
    "\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "def Folder_Path(q):\n",
    "    os.makedirs(\"./temp/\"+str(q)+'/',exist_ok=True)\n",
    "    return \"./temp/\"+str(q)+'/'\n",
    "\n",
    "for q in range(4075,4800):\n",
    "    get_picture=[]\n",
    "    time.sleep(3)\n",
    "\n",
    "#     for q1 in range(1,55):\n",
    "#         search_url=\"https://img.erogazou.gallery/articles/{}/b/{:02d}.jpg\".format(q,q1)\n",
    "    for q1 in range(700,720):\n",
    "        search_url=\"https://img.erogazou.gallery/articles/{}/b/{:02d}.jpg\".format(q,q1)\n",
    "        get_picture.append(search_url)\n",
    "    Target_path = Folder_Path(q)    \n",
    "        \n",
    "        get_picture.append(search_url)\n",
    "    Target_path = Folder_Path(q)\n",
    "\n",
    "    for img in get_picture:\n",
    "        try:\n",
    "            image_content=requests.get(img).content\n",
    "        except Exception as e:\n",
    "            print(f\"Error - could not download {img} - {e}\") \n",
    "        try:\n",
    "            image_file = io.BytesIO(image_content)\n",
    "            image = Image.open(image_file).convert(\"RGB\")\n",
    "            file_path = os.path.join(Target_path,hashlib.sha1(image_content).hexdigest()[:10] + \".jpg\",)\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                    image.save(f, \"JPEG\", quality=100)\n",
    "        except Exception as e:\n",
    "                print(f\"ERROR - Could not save {img} - {e}\")\n",
    "\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Webページを取得して解析する\n",
    "\n",
    "# load_url = \"https://news.idolsenka.net/?p=709#more-709\"\n",
    "load_url = \"https://www.investors.com/data-tables/timesaver-table-feb-15-2022/\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "# HTML全体を表示する\n",
    "print(soup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
